---
title: "![](DEEP_Header.png){out.width=1400px}"
date: "<i> Report Created: `r format(Sys.Date(), '%B %Y')`</i>"
output:
  html_document:
    css: "style.css"
    toc: true
    toc_float: true
    toc_depth: 3
    toccolor: black
    theme: lumen
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(
  comment = '', fig.width = 11, fig.height = 7, warning= FALSE, message = FALSE, fig.align ="left")
```

```{r LIBRARY,include=FALSE}
#Check at line 46-48 for to change directory to match your file path before running!!

#Install packages if not done already 
#install.packages("dplyr")
#install.packages("lubridate")
#install.packages("plotly")
#install.packages("tidyverse")
#install.packages("htmlwidgets")
#install.packages("htmltools")
#install.packages("gt")
#install.packages("padr")
#install.packages("zoo")
#install.packages("magrittr")
library(dplyr)
library(lubridate)
library(plotly)
library(tidyverse)
library(htmlwidgets)
library(htmltools)
library(gt)
library(padr)
library(zoo)
library(magrittr)
```

```{r format, echo=FALSE}
setwd("P:/Community Monitoring/Working/AQMesh/Data_Files")
dir <- "P:/Community Monitoring/Working/AQMesh"
dir_files <-"P:/Community Monitoring/Working/AQMesh/Data_Files"

#listing all files
all_files <- list.dirs(path = dir_files, full.names = TRUE)

#reformatting the AQMesh gas files, this will make a list of files that contain that file pattern (so make sure its that file pattern!)
gas_files <- list.files(path = all_files, pattern = "aqmeshgasdata")
AQ_gaslist <- lapply(gas_files, read.csv)

for (i in 1:length(AQ_gaslist)){
  #Subsetting based on the needed columns
  AQ_gaslist[[i]] <- AQ_gaslist[[i]][c("reading_datestamp", "co_prescaled", "no_prescaled", "no2_prescaled", "o3_prescaled", "eo_prescaled", "aux1_prescaled", "aux2_prescaled", "humidity")] 
}

#binding the rows!
AQ_gas <- bind_rows(AQ_gaslist)

#timestamp
AQ_gas$timestamp <- as.POSIXct(AQ_gas$reading_datestamp, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
AQ_gas$timestamp <- with_tz(AQ_gas$timestamp, "EST")

#summarizing by hour but only if there is 75 percent of the data
completeness_threshold <- 0.75
AQ_gas_avg <- AQ_gas %>%
  mutate(hour = format(timestamp, "%Y-%m-%d %H:00:00")) %>%
  group_by(hour) %>%
  summarize(AQ_co = if (sum(!is.na(co_prescaled))/4 >= 
                         completeness_threshold) mean(co_prescaled, na.rm = TRUE) else NA_real_,
            AQ_no = if (sum(!is.na(no_prescaled))/4 >= 
                         completeness_threshold) mean(no_prescaled, na.rm = TRUE) else NA_real_,
            AQ_no2 = if (sum(!is.na(no2_prescaled))/4 >= 
                          completeness_threshold) mean(no2_prescaled, na.rm = TRUE) else NA_real_,
            AQ_o3 = if (sum(!is.na(o3_prescaled))/4 >= 
                         completeness_threshold) mean(o3_prescaled, na.rm = TRUE) else NA_real_,
            AQ_eo = if (sum(!is.na(eo_prescaled))/4 >= 
                         completeness_threshold) mean(eo_prescaled, na.rm = TRUE) else NA_real_,
            AQ_ws = if (sum(!is.na(aux1_prescaled))/4 >= 
                         completeness_threshold) mean(aux1_prescaled, na.rm = TRUE) else NA_real_,
            AQ_wd = if (sum(!is.na(aux2_prescaled))/4 >= 
                         completeness_threshold) mean(aux2_prescaled, na.rm = TRUE) else NA_real_)
AQ_gas_avg$hour <- as.character(format(AQ_gas_avg$hour))
names(AQ_gas_avg)[1] <- "Date_Time"

#pulling and reformatting the AQMesh pm files, same as gas just pm 
pm_files <- list.files(path = all_files, pattern = "aqmeshpmdata")
AQ_pmlist <- lapply(pm_files, read.csv)

for (i in 1:length(AQ_pmlist)){
  #Subsetting based on the needed columns
  AQ_pmlist[[i]] <- AQ_pmlist[[i]][c("reading_datestamp", "pm10_prescale", "pm2_5_prescale", "pm1_prescale", "temperature_f", "humidity")] 
}

#binding the rows!
AQ_pm <- bind_rows(AQ_pmlist)

#timestamp!
AQ_pm$timestamp <- as.POSIXct(AQ_pm$reading_datestamp, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
AQ_pm$timestamp <- with_tz(AQ_pm$timestamp, "EST")

#summarizing by hour
completeness_threshold <- 0.75

AQ_pm_avg <- AQ_pm %>%
  mutate(hour = format(timestamp, "%Y-%m-%d %H:00:00")) %>%
  group_by(hour) %>%
  summarize(AQ_pm10 = if (sum(!is.na(pm10_prescale))/4 >= 
                           completeness_threshold) mean(pm10_prescale, na.rm = TRUE) else NA_real_,
            AQ_pm2.5 = if (sum(!is.na(pm2_5_prescale))/4 >= 
                            completeness_threshold) mean(pm2_5_prescale, na.rm = TRUE) else NA_real_,
            AQ_pm1 = if (sum(!is.na(pm1_prescale))/4 >= 
                          completeness_threshold) mean(pm1_prescale, na.rm = TRUE) else NA_real_,
            AQ_tempF = if (sum(!is.na(temperature_f))/4 >= 
                            completeness_threshold) mean(temperature_f, na.rm = TRUE) else NA_real_,
            AQ_RH = if (sum(!is.na(humidity))/4 >= 
                         completeness_threshold) mean(humidity, na.rm = TRUE) else NA_real_)
AQ_pm_avg$hour <- as.character(format(AQ_pm_avg$hour))
names(AQ_pm_avg)[1] <- "Date_Time"

#Pulling in East Hartford reference files, even though its one file I'm still doing list, just so you have the option to just pull new files
#save files from Envista as csv, just makes it a tad easier in R
EH_files <- list.files(path = all_files, pattern = "East_Hartford")
EH_list <- lapply(EH_files, read.csv, skip = 2)

for (i in 1:length(EH_list )){
  #removing that random row that isnt needed
  EH_list[[i]] <- EH_list[[i]][-c(1),]
}
EH <- bind_rows(EH_list)

#Timestamp
EH$Date_Time  <- as.POSIXct(EH$Date...Time, format = "%m/%d/%Y %H:%M", TZ= "UTC")
EH <- pad(EH)
EH$Date_Time <- as.character(format(EH$Date_Time))
EH <- EH[,-1]

#Making one data frame with the data merged by hour 
aq <- merge(AQ_gas_avg, AQ_pm_avg, by="Date_Time")
all <- merge(EH, aq, by="Date_Time", all.x = TRUE)

#Writing a csv for all data, this is what will be downloadable on the markdown
write.csv(all, paste0(dir, "/CT_AQMeshData.csv"), row.names=FALSE, na = "")

#Making individual dataframes to compare each pollutant/ reformat pollutants
O3 <- all[, c("Date_Time", "AQ_o3", "O3")]
names(O3) <- c("Date_Time", "AQ", "EH")
NO2 <- all[,c("Date_Time", "AQ_no2", "NO2")]
names(NO2) <- c("Date_Time", "AQ", "EH")
PM25 <- all[,c("Date_Time", "AQ_pm2.5", "T640_PM25")]
names(PM25) <- c("Date_Time", "AQ", "EH")
PM10 <- all[,c("Date_Time", "AQ_pm10", "T640_PM10")]
names(PM10) <- c("Date_Time", "AQ", "EH")
WS <- all[,c("Date_Time", "AQ_ws", "WSsonic")]
names(WS) <- c("Date_Time", "AQ", "EH")
WD <- all[,c("Date_Time", "AQ_wd", "WDsonic")]
names(WD) <- c("Date_Time", "AQ", "EH")
Temp <- all[,c("Date_Time", "AQ_tempF", "TMPOS")] 
names(Temp) <- c("Date_Time", "AQ", "EH")
RH <- all[,c("Date_Time", "AQ_RH", "RH")] 
names(RH) <- c("Date_Time", "AQ", "EH")

#Quickly changing AQ Temp to Celsius
Temp$AQ <- as.numeric(Temp$AQ)
Temp$AQ <- ((Temp$AQ - 32) * 5/9)

#Adding a column with the data type, sets up for the loops
O3$Data_Type <- "O₃"
NO2$Data_Type <- "NO₂"
PM25$Data_Type <-"PM\u2082.\u2085"
PM10$Data_Type <- "PM\u2081\u2080"
WS$Data_Type <- "Wind Speed"
WD$Data_Type <- "Wind Direction"
Temp$Data_Type <- "Temperature"
Temp$units <- "(°C)"
RH$Data_Type <- "Relative Humidity"
RH$units <- "(%)"

#Binding gases then pm into a dataframe
gases <- rbind(O3,NO2)
pm <- rbind(PM25,PM10)
met <- rbind(Temp,RH)

#Adding a units column 
gases$units <- "(ppb)"
pm$units <- "(µg/m³)"

#Combined dataframe for graphing!
comb_full <- rbind(gases, pm, met)

#First pulling/renaming the values that are < -100 since we know these are not tracking data! 
#mostly just so these dont get counted as outliers 
comb_full$AQ <-replace(comb_full$AQ, comb_full$AQ < -100, NA)

#This is detecting outliers based on the IQR Method
#It basically will find values in Q1 and Q3 multiple by 1.5 and fence off those values, BUT it will rename the outlier "Outlier"
#Renaming so you can later count specifically what was counted as an outlier 
detect_outlier <- function(x,iqtimes=1.5) {
  # calculate first quantile
  Quantile1 <- quantile(x, probs=.25, na.rm = T)
  # calculate third quantile
  Quantile3 <- quantile(x, probs=.75, na.rm = T)
  # calculate inter quartile range
  IQR = Quantile3-Quantile1
  # return true or false
  outiers <- x > Quantile3 + (IQR*iqtimes) | x < Quantile1 - (IQR*iqtimes)
  x[which(outiers)] <- "outlier"
  return(x)
}

#designating what columns to clean, goes based on what is numeric
cols_to_clean <- names(comb_full)[sapply(comb_full, is.numeric)]

#Running the function
comb_out <- comb_full %>% group_by(Data_Type) %>%
  mutate(across(cols_to_clean , ~detect_outlier(.,iqtimes=1.5)))

#Only keeping numeric data, basically this changes outlier/no data to "NA" for this specific dataframe, so it can be graphed
#graphs get very mad about non-numeric data
comb <- comb_out
comb$AQ <- as.numeric(comb$AQ)
comb$EH <- as.numeric(comb$EH)

#dropping NAs from the date column, there uselessly are not any but this could effect graphing
comb_out <- comb_out %>% drop_na(Date_Time)

#This adds the quarter an year, but I dont like the format so also changing that
comb$Quarter <- as.yearqtr(comb$Date_Time, format = "%Y-%m-%d")
comb$Quarter <- format(comb$Quarter, format = "%q (%Y)")

#Truncating the values 
comb$AQ <- trunc(comb$AQ * 10) / 10

```

```{r csv link, echo =FALSE}
#This is how the csv is inserted as a link, then is added to the text below
readBin("CT_AQMeshData.csv", "raw", file.info("CT_AQMeshData.csv")$size) %>% 
  openssl::base64_encode() -> encoded
```

An AQMesh air quality monitoring sensor was installed at a monitoring site in East Hartford, Connecticut to evaluate its performance tracking gas, particulate and meteorology data over a year long time frame. Hourly data for O~3~, NO~2~, PM~2.5~, PM~10~, temperature and relative humidity were compared to reference monitors located at the same site. The full downloadable dataset used is located here <a download="CT_AQMeshData.csv" href="`r sprintf('data:text/csv;base64,%s', encoded)`">Download CSV</a>. 

# Sensor Details 
## AQMesh Specifications
<table border="2" style="border-collapse: collapse; border-color: black;">
<tr style="background-color: #0D2C6C; color: white; text-align: left;">
<td style="font-size: 16px; padding: 8px; border: 1px solid black;"><b>Possible Configuration</td>
<td style="font-size: 16px; padding: 8px; border: 1px solid black;"><b>Evaluated Configuration</td>
<td style="font-size: 16px; padding: 8px; border: 1px solid black;"><b>Cost</td>
<td style="font-size: 16px; padding: 8px; border: 1px solid black;"><b>Data Access</td>
<td style="font-size: 16px; padding: 8px; border: 1px solid black;"><b>Power Supply</td>
<td style="font-size: 16px; padding: 8px; border: 1px solid black;"><b>Considerations</td>
<td style="font-size: 16px; padding: 8px; border: 1px solid black;"><b>Reference Monitors Compared</b></td>
</tr>
</tr>
<tr style="background-color: #white; color: black;">
<td style=" text-align: left; vertical-align:top; padding: 8px; border: 1px solid black;"> <b> Particulates: </b> PM~1~, PM~2.5~, PM~10~ <br> <b> Gases: </b> NO, NO~2~, O~3~, CO, CO~2~, TVOC, SO~2~, EtO <br> <b> Meteorology: </b> WS/WD, Baro Pressure, Temp, RH </td>
<td style ="vertical-align:top; padding: 8px; border: 1px solid black;"><b> Particulates: </b> PM~2.5~, PM~10~ <br> <b> Gases: </b> O~3~, NO~2~ <br> <b> Meteorology: </b> Temp, RH
</td>
<td style ="vertical-align:top; padding: 8px; border: 1px solid black;"> <b> Sensor: </b> $6,663.00 <br> <b> Sonic Anemometer: </b> $4,998.00 <br> <b> Solar Configuration: </b> $1,967.00 <br> <b> Cellular Communications (Annual): </b> $420.00 <br> <b> API or Dashboard: </b> $1,260.00
</td>
<td style ="vertical-align:top; padding: 8px; border: 1px solid black;"> Basic Download (CSV), API, Web Application <br> (Data stored locally as backup only, inaccessible to customer)
</td>
<td style ="vertical-align:top; padding: 8px; border: 1px solid black;"> Smart Solar Pack, Rechargeable NiMH battery, Mains DC Power
</td>
<td style ="vertical-align:top; padding: 8px; border: 1px solid black;"> <b> Time Resolution: </b> 1 minute to 1 hour intervals <br> <b> Dimensions: </b> 430 (H) x 220 (W) x 170 (L) mm (including antenna) <br> <b> Weight: </b> 2-2.7 kg
</td>
<td style ="vertical-align:top; padding: 8px; border: 1px solid black;"> <b> O~3~:</b> Teledyne N400 
<br>
<b> NO~2~:</b> Teledyne T500U
<br>
<b> PM~2.5~ & PM~10~:</b> Teledyne API T640X
<br>
<b> Temp & RH: </b> Climatronix & Vaisala via AutoMet 580
</td>
</tr>
</table>

## Setup
```{r, echo = FALSE, out.width = '110%', out.height= '110%', fig.show = 'hold', fig.align = 'center', fig.cap=' '}
#adding the photo of the sensor
knitr::include_graphics(c("AQMesh.png"))
```
## Methods 
  One AQMesh pod was installed at CT DEEP’s McAuliffe Park Ambient Air Monitoring Station following collocation requirements from Ambilabs: sited greater than 0.5m above roof level, 1 meter horizontally from reference inlets, and free of any obstructions that could impact the free movement of air. The AQMesh was powered by a solar panel and battery pack purchasable from Ambilabs, and wind speed and direction were provided through a Davis anemometer.  Data from the AQMesh pod were streamed to a central DAS over a 3G cellular connection and averaged into 15-minute intervals. CT DEEP accessed data through an API feed. Data were averaged into hour-intervals for comparison with regulatory O~3~, NO~2~, PM~2.5~, PM~10~, and meteorological instrumentation. “Prescale” data fields were used in analysis. AQMesh offers linear regression analysis for collocated pods, but regressions were conducted by CT DEEP and applied to data manually to preserve corrected vs uncorrected data comparisons. 

<br>

```{r timeseries, results = 'asis', echo = FALSE}
#This makes a list of timeseries plots with two loops, first through the quarter/year then through datatype
#Output is a timeseries graph for each quarter/year for each measurement 
timeseries = list()

idx <- 1
for (i in unique(comb$Quarter)){
  
  i_comb <- subset(comb, comb$Quarter==i)
  
for (j in unique(i_comb$Data_Type)){
  
  j_comb <- subset(i_comb, i_comb$Data_Type==j)
  j_comb$Date_Time <- as.POSIXct(j_comb$Date_Time)
  
   plot_name <- paste0("Q_", i, "_", j)

  timeseries[[plot_name]] <- plot_ly(data= j_comb, x = ~Date_Time) %>%
    add_lines(y = ~EH, name = "Reference", line = list(color = "black"), opacity = 0.9,
    hoverinfo = 'text', text = ~paste0(format(Date_Time, "%m/%d/%y %H:%M"),"<br>","Reference: ", EH)) %>%
    add_lines(y = ~AQ, name = "AQMesh", line = list(color = "blue"), opacity = 0.6,
    hoverinfo = 'text', text = ~paste0(format(Date_Time, "%m/%d/%y %H:%M"),"<br>","AQMesh: ", AQ)) %>%
    layout(title = list(text = paste0("AQMesh Sensor: ", unique(j_comb$Data_Type)," Comparision",
                                      "<br>",
                                      "<sup>", 
                                       "Quarter ", unique(i_comb$Quarter),  "<sup>")),
           legend = list(orientation = 'h', title=list(text="Monitor Type:")), 
           xaxis = list(title = " ",
                        type = 'date',
                        tickformat = "%B %d <br>%Y"),
           annotations = list(x = 0.60, y = -0.17, text = paste0("<i>  *Outliers removed from AQMesh dataset using IQR Method.                                                                  </i>"), 
      showarrow = F, xref='paper', yref='paper', 
      xanchor='right', yanchor='auto', xshift=0, yshift=0,
      font=list(size=12, color="grey")),
           yaxis = list(title = paste0(unique(j_comb$Data_Type), " ", unique(j_comb$units)), rangemode = 'tozero'))
  idx <- idx + 1

}}

```

# Timeseries Comparison

AQMesh hourly data for O~3~, NO~2~, PM~2.5~, PM~10~, temperature and relative humidity were compared by quarter to reference values (Q1: January-March, Q2: April-June, Q3: July-September, Q4: October-December).

## O~3~{.tabset .tabset-fade .tabset-pills}

### Quarter 3 (2023)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_3 (2023)_O₃']]
```

### Quarter 4 (2023)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_4 (2023)_O₃']]
```
### Quarter 1 (2024)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_1 (2024)_O₃']]
```
### Quarter 2 (2024)
```{r results = 'asis', echo = FALSE}
timeseries[['Q_2 (2024)_O₃']]
```

## NO~2~ {.tabset .tabset-fade .tabset-pills}
### Quarter 3 (2023)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_3 (2023)_NO₂']]
```

### Quarter 4 (2023)
```{r results = 'asis', echo = FALSE}
timeseries[['Q_4 (2023)_NO₂']]
```

### Quarter 1 (2024)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_1 (2024)_NO₂']]
```

### Quarter 2 (2024)
```{r results = 'asis', echo = FALSE}
timeseries[['Q_2 (2024)_NO₂']]
```
## PM~2.5~ {.tabset .tabset-fade .tabset-pills}
### Quarter 3 (2023)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_3 (2023)_PM₂.₅']]
```
### Quarter 4 (2023)
```{r results = 'asis', echo = FALSE}
timeseries[['Q_4 (2023)_PM₂.₅']]
```
### Quarter 1 (2024)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_1 (2024)_PM₂.₅']]
```
### Quarter 2 (2024)
```{r results = 'asis', echo = FALSE}
timeseries[['Q_2 (2024)_PM₂.₅']]
```

## PM~10~ {.tabset .tabset-fade .tabset-pills}
### Quarter 3 (2023)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_3 (2023)_PM₁₀']]
```

### Quarter 4 (2023)
```{r results = 'asis', echo = FALSE}
timeseries[['Q_4 (2023)_PM₁₀']]
```

### Quarter 1 (2024)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_1 (2024)_PM₁₀']]
```

### Quarter 2 (2024)
```{r results = 'asis', echo = FALSE}
timeseries[['Q_2 (2024)_PM₁₀']]
```

## Temperature {.tabset .tabset-fade .tabset-pills}
### Quarter 3 (2023)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_3 (2023)_Temperature']]
```
### Quarter 4 (2023)
```{r results = 'asis', echo = FALSE}
timeseries[['Q_4 (2023)_Temperature']]
```
### Quarter 1 (2024)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_1 (2024)_Temperature']]
```
### Quarter 2 (2024)
```{r results = 'asis', echo = FALSE}
timeseries[['Q_2 (2024)_Temperature']]
```

## Relative Humidity {.tabset .tabset-fade .tabset-pills}
### Quarter 3 (2023)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_3 (2023)_Relative Humidity']]
```
### Quarter 4 (2023)
```{r results = 'asis', echo = FALSE}
timeseries[['Q_4 (2023)_Relative Humidity']]
```
### Quarter 1 (2024)
```{r, results = 'asis', echo = FALSE}
timeseries[['Q_1 (2024)_Relative Humidity']]
```
### Quarter 2 (2024)
```{r results = 'asis', echo = FALSE}
timeseries[['Q_2 (2024)_Relative Humidity']]
```
<br/>

```{r, results = 'asis', echo = FALSE}
#correlation plot
#Removing nas for this one because correlation plots wont just ignore them
comb <- na.omit(comb)

correlation <- tagList()
idx <- 1
for (i in unique(comb$Data_Type)){
  
  i_comb <- subset(comb, comb$Data_Type==i)

  lm_calc <- lm(i_comb$AQ ~ i_comb$EH)
  slope <- coef(lm_calc)[2]
  y <- coef(lm_calc)[1]
  r <- summary(lm_calc)$r.squared
  
corrplot_name <- paste0(i)
  
correlation[[corrplot_name]] <-plot_ly(data = i_comb) %>% 
  add_markers(x = i_comb$EH, y = i_comb$AQ, name = " ", marker = list(color = "lightsteelblue",
                                                                            line = list(color = "#0D2C6C",width = 1.3))) %>%
  add_lines(x = i_comb$EH, y = fitted(lm(i_comb$AQ ~ i_comb$EH)),name = " ", line=list(color = "black", width= 1)) %>%
  layout(showlegend = F, 
         title = list(text = paste0("AQMesh Sensor: ", 
                                                   unique(i_comb$Data_Type)," Correlation ", unique(i_comb$units), "<br>",
                                                   "<sup>", "y=", round(slope, 3), "x + ", round(y,3), "  ", "R\u00b2", "=", round(r,3),"<sup>")),
         annotations = list(x = 0.60, y = -0.07, text = paste0("<i>  *Outliers removed from AQMesh dataset using IQR Method.                                                                  </i>"), 
      showarrow = F, xref='paper', yref='paper', 
      xanchor='right', yanchor='auto', xshift=0, yshift=0,
      font=list(size=12, color="grey")),
         xaxis = list(title = "Reference", rangemode = 'tozero'), 
         yaxis = list(title = "AQMesh", rangemode = 'tozero'))
idx <- idx + 1

}

```

# Correlation Comparison
## AQMesh and Reference Correlation
### Gases {.tabset .tabset-fade .tabset-pills}
#### O~3~
```{r, results = 'asis', echo = FALSE}
correlation[['O₃']]
```
#### NO~2~
```{r, results = 'asis', echo = FALSE}
correlation[['NO₂']]
```

### Particulates {.tabset .tabset-fade .tabset-pills}
#### PM~2.5~
```{r, results = 'asis', echo = FALSE}
correlation[['PM₂.₅']]
```
#### PM~10~
```{r, results = 'asis', echo = FALSE}
correlation[['PM₁₀']]
```

### Meteorology {.tabset .tabset-fade .tabset-pills}
#### Temperature
```{r, results = 'asis', echo = FALSE}
correlation[['Temperature']]
```
#### Relative Humidity
```{r, results = 'asis', echo = FALSE}
correlation[['Relative Humidity']]
```

```{r, echo=FALSE, results = 'asis'}
#finding outliers
Outlier <- comb_out %>%
  # Create a new column to check if "outlier" is present
  mutate(Contains_Outlier = grepl("outlier", AQ, ignore.case = TRUE)) %>%
  # Group by Data_Type
  group_by(Data_Type) %>%
  # Summarize the percentage of rows containing "outlier"
  summarize(
    Total_Count = n(),
    Outlier_Count = sum(Contains_Outlier),
    Percentage_Outlier = (Outlier_Count / Total_Count) * 100
  )

#rounding the data
Outlier <- Outlier %>% mutate(across(where(is.numeric), ~ round(., 2)))

#Finding the na percent
na <- comb_out %>%
  group_by(Data_Type) %>%
  summarise(
    total = n(),
    na_count = sum(is.na(AQ)),
    na_percent = (na_count / total) * 100
  )
na$na <- 100 - na$na_percent
na <- na[c("Data_Type", "na")]
names(na)[2] <- "complete"

#rounding
na <- na %>% mutate(across(where(is.numeric), ~ round(., 1)))

#Root mean square error
comb <- na.omit(comb)
rmse <- comb %>%
  group_by(Data_Type) %>%
 summarize(
    RMSE = sqrt(mean((EH - AQ)^2))
  )
rmse <- rmse %>% mutate(across(where(is.numeric), ~ round(., 2)))
  
#merging everything together 
Outlier <- merge(rmse, Outlier, by = "Data_Type", all.x = TRUE)
Outlier <- merge(Outlier, na, by = "Data_Type", all.x = TRUE)

#setting up data for a table
EH_AQ <- do.call(rbind, lapply(unique(comb$Data_Type), function(d) {
  EH_AQ_model <- lm(AQ ~ EH, data = comb[comb$Data_Type == d,])
  data.frame(Data_Type = d, Intercept = coef(EH_AQ_model)[1],
             Slope = coef(EH_AQ_model)[2], r_squared = summary(EH_AQ_model)$r.squared,
             row.names = NULL)
}))

EH_AQ <- EH_AQ %>% mutate(across(where(is.numeric), ~ round(., 2)))

#binding them for the table!
table <- merge(EH_AQ, Outlier, by = "Data_Type", all.x = TRUE)
table <- table[c("Data_Type","r_squared", "Slope", "Intercept","RMSE", "Percentage_Outlier", "complete")]
table$complete[is.na(table$complete)] <- "100"

AQ_RDS <- table[c("Data_Type", "r_squared")]
saveRDS(AQ_RDS, file="AQ_RDS.rds")

# Define the range for the slope
slope_min <- 1.0 - 0.35
slope_max <- 1.0 + 0.35
slopeminO3 <- 1.0 - 0.20
slopemaxO3 <- 1.0 + 0.20
  
#making a table!
table1 <- table |>
  gt(
    rowname_col = "Data_Type")|>
  cols_width(everything() ~ px(105)) |>
  tab_header(
    title = ("AQMesh"),
    subtitle = ("Sensor vs. Reference Correlations"))|>
  cols_label(
    r_squared = ("R\u00b2"),
    Slope = ("Slope"),
    Intercept = ("Intercept"),
    Percentage_Outlier = ("Outlier Percentage"),
    complete= ("Data Completeness"))|>
cols_align(
  align = ("center"),
  columns = everything())|>
sub_missing(
  missing_text = "0.00")|>
tab_footnote(
    footnote =("Bolded values indicate the target was met for PM and gas data according to the recommended EPA performance metrics."), 
    locations = cells_title("subtitle"))|>
  tab_options(
      footnotes.font.size = px(11))|>
tab_style(
      style = list(cell_text(weight = "bold")), 
      locations = cells_body(
      columns = vars(r_squared),
      rows = Data_Type == "O₃" & r_squared >= 0.8))|>
tab_style(
      style = list(cell_text(weight = "bold")), 
      locations = cells_body(
      columns = vars(r_squared),
      rows = Data_Type %in% c("NO₂","PM₂.₅","PM₁₀")  & r_squared >= 0.7))|>
tab_style(
      style = list(cell_text(weight = "bold")), 
      locations = cells_body(
      columns = vars(Slope),
      rows = Data_Type == "O₃"  &  Slope >= slopeminO3 & Slope <= slopemaxO3))|>
tab_style(
      style = list(cell_text(weight = "bold")), 
      locations = cells_body(
      columns = vars(Slope),
      rows = Data_Type %in% c("NO₂","PM₂.₅","PM₁₀")  &  Slope >= slope_min & Slope <= slope_max))|>
tab_style(
      style = list(cell_text(weight = "bold")), 
      locations = cells_body(
      columns = vars(Intercept),
      rows = Data_Type %in% c("NO₂", "O₃", "PM₂.₅") & Intercept > -5 & Intercept < 5))|>
tab_style(
      style = list(cell_text(weight = "bold")), 
      locations = cells_body(
      columns = vars(Intercept),
      rows = Data_Type %in% c("PM₁₀") & Intercept > - 10 & Intercept < 10))|>
tab_style(
      style = list(cell_text(weight = "bold")), 
      locations = cells_body(
      columns = vars(RMSE),
      rows = Data_Type == "O₃" & RMSE <= 5))|>
tab_style(
      style = list(cell_text(weight = "bold")), 
      locations = cells_body(
      columns = vars(RMSE),
      rows = Data_Type == "NO₂" & RMSE <= 15))|>
tab_style(
      style = list(cell_text(weight = "bold")), 
      locations = cells_body(
      columns = vars(RMSE),
      rows = Data_Type ==  "PM₂.₅" & RMSE <= 7))|>
tab_style(
      style = list(cell_text(weight = "bold")), 
      locations = cells_body(
      columns = vars(RMSE),
      rows = Data_Type ==  "PM₁₀" & RMSE <= 14))


gtsave(table1, "table1.png")

```
# Results Summary

```{r,fig.align = 'left', results='asis', echo = FALSE}
knitr::include_graphics("table1.png")
```
<i> \*Outliers were defined from the AQMesh dataset using the IQR Method (all data points more than 1.5 below the the lower bound quartile or above the upper bound quartile). </i>

## Discussion 
  PM~10~ readings were significantly impacted by the “fog affect” that typically occurs at dawn between the hours of 3-6am EST during summer months. AQMesh flags coarse particulate readings with the code “deliquescence” if measured RH values are above a threshold, however those flags were not largely applied in our dataset. During these fog events, PM~10~ values were reported at anomalously high values due to interference with the optical particle counter. These outliers were removed from the dataset using the IQR method, in which all data points 1.5 beyond the lower and upper bound quartiles were excluded. Due to the limitations of the OPCs, data from these fog windows- especially in summer months- would have to be excluded from a dataset. Several studies suggest a heatded inlet would decrease this humidity sensitivity, however that was not evaluated due to our solar configuration.  

  With outliers removed from the dataset, PM~2.5~ was the only parameter which fell within acceptable levels in comparison to EPA Air Sensor Performance Targets. PM~2.5~ shows significant correlation through linear regression models and acceptable offset and RMSE, with slope just outside of the target range. With outliers excluded, PM~10~ data show acceptable offset and RMSE, but with slope and R^2^ beyond target ranges. O~3~ data are just outside of targets for slope, R^2^ and RMSE, but display an acceptable offset. NO~2~ measurements showed weak to non-correlation with the regulatory instrument. Temperature and RH measurements in the pod were expected to vary slightly from station measurements due to the location of the sensors within the sensor housing. However, results show that temperature and RH correlate well with ambient measurements and can be expected to track near ambient levels. Data capture is 100% during the testing window, with no significant (greater than 7 minutes) power disruptions. The only data unavailable for analysis were PM~2.5~ and PM~10~ measurements flagged for deliquescence in the AQMesh DAS.  

# Contact Information
Questions on Connecticut community based monitoring: DEEP.AirMonitoring@ct.gov <br>
Questions on creating this report: Jessica.Landry@ct.gov
